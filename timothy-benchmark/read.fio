[global]
# I/O Engine and Behavior:
ioengine=libaio ;Uses the Linux native asynchronous I/O engine, libaio, for efficient I/O operations.
direct=1 ;Bypass the operating system cache and read data directly from the storage device. This can provide a more realistic measurement of storage performance as it avoids any caching effects.
fadvise_hint=0 ;Don't provide file access hints to the operating system.
verify=0 ;Disable data verification. This can improve performance, but should be enabled if data integrity is critical.
rw=read ;Perform sequential read operations.
bs=1M ;Use a block size of 1MB for I/O operations.
iodepth=64 ;Allow up to 64 outstanding I/O requests to be issued at the same time. This can improve I/O performance, especially when dealing with high-latency storage.
invalidate=1 ;Invalidate the data in the cache before each read. This ensures that each read is from the storage device, not the cache.
# Timing and Load:
ramp_time=2m ;Gradually increase the I/O load over a period of 1 minute. This helps to avoid shocking the storage system at the start of the test.
runtime=10m ;Run the test for 10 minutes. The `ramp_time` is included
;startdelay=2m ;Wait for 2 minutes before starting the test. This can be used to stagger the start times of multiple Fio jobs, or to allow the storage device to warm up.
time_based=1 ;Run the test based on time, rather than the number of I/O operations completed.
# Files and Jobs:
nrfiles=1 ;Use one file per job.
thread=1 ;Use one thread per job.
fsync=1 ;Perform an fsync() operation after every write, flushing data to the storage device.
openfiles=1 ;Keep one file open per job.
group_reporting=1 ;Aggregate the results of all jobs into a single report.
allrandrepeat=1 ;Use the same random access pattern for all jobs.
filename_format=$jobname.$jobnum.$filenum ;Defines the format used to generate filenames.
[single_group]
bs=${BLOCK_SIZE}
filesize=${FILE_SIZE}
directory=/data/${FILE_SIZE}
rw=${MODE}
numjobs=${NUMJOBS}
filename_format=${FILE_SIZE}_${MODE}.$jobnum.$filenum
